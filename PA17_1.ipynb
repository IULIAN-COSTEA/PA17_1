{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application III: Comparing Classifiers\n",
    "\n",
    "**Overview**: In this practical application, your goal is to compare the performance of the classifiers we encountered in this section, namely K Nearest Neighbor, Logistic Regression, Decision Trees, and Support Vector Machines.  We will utilize a dataset related to marketing bank products over the telephone.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset source location and context description\n",
    "\n",
    "Our dataset comes from the UCI Machine Learning repository [link](https://archive.ics.uci.edu/ml/datasets/bank+marketing).  The data is from a Portugese banking institution and is a collection of the results of multiple marketing campaigns.  We will make use of the article accompanying the dataset [here](CRISP-DM-BANK.pdf) for more information on the data and features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset details \n",
    "This marketing dataset is related to 17 marketing campaigns that occurred between May 2008 and November 2010, run by a Portuguese bank that used its own contact-center to do directed marketing campaigns for a long-term deposit application with good interest rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, EditedNearestNeighbours\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "from warnings import filterwarnings \n",
    "filterwarnings('ignore')\n",
    "\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/bank-additional-full.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/bank-additional-full.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/bank-additional-full.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/bank-additional-full.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Input variable description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input variables:\n",
    "# # bank client data:\n",
    "# 1 - age (numeric)\n",
    "# 2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "# 3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "# 4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "# 5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "# 6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "# 7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "# # related with the last contact of the current campaign:\n",
    "# 8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "# 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "# 10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "# 11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "# # other attributes:\n",
    "# 12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "# 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "# 14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "# 15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "# # social and economic context attributes\n",
    "# 16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "# 17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "# 18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "# 19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "# 20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "# Output variable (desired target):\n",
    "# 21 - y - has the client subscribed a term deposit? (binary: 'yes','no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the dataset\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cleaning up duplicates and missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values and duplicates\n",
    "print(\"=\"*50)\n",
    "print(f'Data frame shape is: {df.shape}')\n",
    "total_missing_values = df.isna().sum().sum()\n",
    "total_duplicates = df.duplicated().sum()\n",
    "print(\"=\"*50)\n",
    "print(f'Total number of missing values is:{total_missing_values}')\n",
    "print(f'Total duplicates value is:{total_duplicates}')\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dropping duplicates\n",
    "print(f'Total duplicates removed:{total_duplicates}')\n",
    "df = df.drop_duplicates().copy()\n",
    "print(f'Data frame shape is: {df.shape}')\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring datatypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing target column from categorical to numerical\n",
    "cat_values = df['y'].unique()\n",
    "print(\"=\" * 65)\n",
    "print(f'Detected non-numerical values for target column [y]:{cat_values}')\n",
    "print(\"=\" * 65)\n",
    "print(f'Replacing non-numerical values for target column [y] ->>>')\n",
    "df['y'] = df['y'].replace({\n",
    "    'no': 0,\n",
    "    'yes': 1,\n",
    "})\n",
    "df['y'] = df['y'].astype(int)\n",
    "num_values = df['y'].unique()\n",
    "print(f'New unique values for target column [y]: {num_values}')\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Exploring and pre-processing categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Calculating the number of plots needed\n",
    "num_plots = len(categorical_columns)\n",
    "\n",
    "# Calculating the number of rows and columns for the subplot grid\n",
    "num_cols = 3\n",
    "num_rows = (num_plots + num_cols - 1) // num_cols\n",
    "\n",
    "# Creating the figure and subplot grid\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows))\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the categorical columns and the axes\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    sns.histplot(x=col, data=df, ax=axes[i], hue='y', multiple='stack')\n",
    "\n",
    "    # Set proper titles and labels\n",
    "    axes[i].set_title(f'Count Distribution of {col.replace(\"_\", \" \").title()}', fontsize=12)\n",
    "    axes[i].set_xlabel(f'{col.replace(\"_\", \" \").title()}', fontsize=12)\n",
    "    axes[i].set_ylabel('Count', fontsize=10)\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# If there are empty subplots, hide them\n",
    "for j in range(num_plots, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout to prevent titles and labels from overlapping\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(\"data/categorical_histplots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning outliers and unknow values\n",
    "\n",
    "df = df[df['loan'] != 'unknown']\n",
    "df = df[df['job'] != 'unknown']\n",
    "df = df[df['housing'] != 'unknown']\n",
    "df = df[df['marital'] != 'unknown']\n",
    "df = df[df['default'] != 'unknown']\n",
    "\n",
    "# List of values to be removed\n",
    "values_to_remove = ['illiterate', 'unknown']\n",
    "\n",
    "# Filter the DataFrame\n",
    "df = df[~df['education'].isin(values_to_remove)] # remove \"illiterate\" and \"unknown\" values that are rather outliers that might affect the models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploring and pre-processing numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features correlation using heatmap\n",
    "plt.figure(figsize=(15, 8))\n",
    "mask_matrix = np.ones_like(df.corr(numeric_only=True), dtype=bool)\n",
    "mask = np.triu(mask_matrix, k=1)\n",
    "sns.heatmap(df.corr(numeric_only=True), mask=mask, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "\n",
    "print(\"=\"*150)\n",
    "print(\"\"\"HEATMAP ANALYSIS: We can observe Multicollinearity between euribor3m, nr.employed, emp.var.rate and cons.price.idx. \n",
    "We will keep only euribor3m because is the only one directly impacting the deposit rates and drop the other rest. \n",
    "cons.conf.idx also has very low correlation with deposits since is more related to the comsumptions rather than savings. Therefore, it will be droped.\"\"\")\n",
    "print(\"=\"*150)\n",
    "# Save the plot to a file\n",
    "plt.savefig(\"data/numerical_heatmap.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping feature columns identified above (including duration as per dataset suggestions)\n",
    "df = df.drop(['nr.employed', 'emp.var.rate'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Determine the number of subplots\n",
    "num_cols = 3  # Number of columns for the grid\n",
    "num_rows = (len(numerical_columns) + num_cols - 1) // num_cols\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5), constrained_layout=True)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each numerical column and create a violin plot\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    sns.violinplot(x=df[col], ax=axes[i], legend=False, color='skyblue')\n",
    "    axes[i].set_title(f'Violin Plot of {col}', fontsize=14, fontweight='bold')\n",
    "    axes[i].set_xlabel(col, fontsize=12)\n",
    "    axes[i].set_ylabel('Density', fontsize=12)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(numerical_columns), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Save the plots to a file\n",
    "plt.savefig(\"data/violin_plots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop or remove outliers from\n",
    "\n",
    "# Drop columns that do not hold relevant information\n",
    "df = df.drop(['pdays', 'previous'], axis=1)\n",
    "\n",
    "columns_to_clean = ['campaign', 'age', 'duration', 'campaign', 'cons.price.idx', 'cons.conf.idx']\n",
    "\n",
    "# Create a copy of the DataFrame to work on\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "print(f\"Original number of rows: {len(df)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Loop through each column and remove outliers\n",
    "for col in columns_to_clean:\n",
    "    # Calculate Q1, Q3, and IQR\n",
    "    Q1 = df_cleaned[col].quantile(0.25)\n",
    "    Q3 = df_cleaned[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define outlier bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filter out the outliers\n",
    "    initial_rows = len(df_cleaned)\n",
    "    df_cleaned = df_cleaned[(df_cleaned[col] >= lower_bound) & (df_cleaned[col] <= upper_bound)]\n",
    "    rows_removed = initial_rows - len(df_cleaned)\n",
    "\n",
    "    print(f\"Outliers removed from '{col}': {rows_removed} rows\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\nTotal rows after cleaning: {len(df_cleaned)}\")\n",
    "print(f\"Total outliers removed: {len(df) - len(df_cleaned)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Replace the original DataFrame with the cleaned one\n",
    "df = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Determine the number of subplots\n",
    "num_cols = 3  # Number of columns for the grid\n",
    "num_rows = (len(numerical_columns) + num_cols - 1) // num_cols\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5), constrained_layout=True)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each numerical column and create a violin plot\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    sns.violinplot(x=df[col], ax=axes[i], legend=False, color='skyblue')\n",
    "    axes[i].set_title(f'Violin Plot of {col}', fontsize=14, fontweight='bold')\n",
    "    axes[i].set_xlabel(col, fontsize=12)\n",
    "    axes[i].set_ylabel('Density', fontsize=12)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(numerical_columns), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Save the plots to a file\n",
    "plt.savefig(\"data/violin_plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-Processing and Experimentation Overview\n",
    "Dataset is very unbalanced and require special treatements to ensure the optimal models results   \n",
    "I will be using two different experiments that will address data unbalace in two different ways:\n",
    "- EXPERIMENT A - will use an oversampling technique(SMOTE) to expand the minority class and rebalance the dataset. Models will be trained on the new balanced dataset. Also, in this experiment SVM model will be trained on a smaller sample due to the resource processing requirements that make the overa process very long\n",
    "- EXPERIMENT B - will use use different undersampling techniques(RandomUnderSampler, NearMiss, EditedNearestNeighbours) to reduce majority class and rebalance the dataset. Models will be trained on the new balanced dataset\n",
    "- Considering the nature of the unbalanced data I will focus on optimizing the F1-Score rather than Accuracy\n",
    "- Once the best model is dentifies an additional optimization process will be conducted to select the optimal probability threshold that will be used for the final predictios of each selected model.\n",
    "- For the best models the parameters will be extracted and the most important features will be listed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setting the Baseline Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train and target dataframes\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "\n",
    "# Splitting data into training and testing sets. I experimented with different test sizes becuase the dataset is big\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Defining the baseline model score\n",
    "dummy_model = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# I will use cross-validation to get a robust estimate of the baseline performance. I will be using F1-Score since dataset is very unballanced\n",
    "baseline_score = cross_val_score(dummy_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(f\"Baseline accuracy set using Majority Class Classifier is: {baseline_score.mean():.4f}\")\n",
    "print(\"=\"*65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pre-procesing data - Define data pre-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Defining the preprocessor\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Applying preprocessor and resampler to the training data\n",
    "# Fitting the preprocessor on the training data and transforming both train and test\n",
    "preprocessor_fitted = preprocessor.fit(X_train)\n",
    "X_train_preprocessed = preprocessor_fitted.transform(X_train)\n",
    "X_test_preprocessed = preprocessor_fitted.transform(X_test)\n",
    "\n",
    "# Applying the resampler to the preprocessed training data\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_resampled_a, y_resampled_a = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Original dataset shape: {X.shape}, {y.shape}\")\n",
    "print(f\"Preprocessed training shape: {X_train_preprocessed.shape}, {y_train.shape}\")\n",
    "print(f\"Resampled training shape: {X_resampled_a.shape}, {y_resampled_a.shape}\")\n",
    "print(f\"Preprocessed testing shape: {X_test_preprocessed.shape}, {y_test.shape}\")\n",
    "print(\"=\"*50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Experiment A - Address unbalanced dataset issue using oversampling of the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting resampled data back to a DataFrame\n",
    "def get_processed_feature_names(preprocessor, X):\n",
    "    output_features = []\n",
    "    output_features.extend(preprocessor.named_transformers_['num'].get_feature_names_out())\n",
    "    output_features.extend(preprocessor.named_transformers_['cat'].get_feature_names_out())\n",
    "    return output_features\n",
    "    \n",
    "processed_feature_names = preprocessor_fitted.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array\n",
    "X_resampled_a_dense = X_resampled_a.toarray() if hasattr(X_resampled_a, 'toarray') else X_resampled_a\n",
    "X_test_preprocessed_dense = X_test_preprocessed.toarray() if hasattr(X_test_preprocessed, 'toarray') else X_test_preprocessed\n",
    "\n",
    "# Creating the DataFrames\n",
    "X_resampled_a_df = pd.DataFrame(X_resampled_a_dense, columns=processed_feature_names)\n",
    "X_test_df = pd.DataFrame(X_test_preprocessed_dense, columns=processed_feature_names)\n",
    "\n",
    "# Defining the models and hyperparameters\n",
    "best_overall_model_exp_a = None\n",
    "best_overall_score_exp_a = 0.0\n",
    "best_overall_params_exp_a = {}\n",
    "best_model_for_final_eval_exp_a = None\n",
    "\n",
    "models = {\n",
    "    'knn': (KNeighborsClassifier(), {'knn__n_neighbors': [3, 5, 7]}),\n",
    "    'logisticregression': (LogisticRegression(max_iter=1000, class_weight='balanced'), {'logisticregression__C': [0.1, 1, 10, 15, 20, 50]}),\n",
    "    'svc': (SVC(probability=True, class_weight='balanced'), {'svc__C': [10, 15, 20], 'svc__kernel': ['rbf']}),\n",
    "    'decisiontreeclassifier': (DecisionTreeClassifier(class_weight='balanced'), {'decisiontreeclassifier__max_depth': [5, 10, 12, 13, 14, 15]})\n",
    "}\n",
    "\n",
    "results = []\n",
    "results_optimized = []\n",
    "\n",
    "# Creating dictionaries to store the predicted probabilities for each experiment\n",
    "y_pred_probas = {}\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    # --- Creating the pipeline ---\n",
    "    pipeline = Pipeline([(name, model)])\n",
    "\n",
    "    # I will select de best model based on F1-score\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=-1, scoring=f1_scorer)\n",
    "    \n",
    "    # ... Conditional logic for SVC training  ...\n",
    "    if name == 'svc':\n",
    "        print(f\"Training SVC on a smaller sample to save time...\")\n",
    "        X_resampled_a_svc, _, y_resampled_a_svc, _ = train_test_split(\n",
    "            X_resampled_a_df, y_resampled_a, test_size=0.2, random_state=42, stratify=y_resampled_a # Modify the test_size for a faster time\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        grid_search.fit(X_resampled_a_svc, y_resampled_a_svc)\n",
    "    else:\n",
    "        print(f\"Training {name.upper()} on the full resampled data.\")\n",
    "        start_time = time.time()\n",
    "        grid_search.fit(X_resampled_a_df, y_resampled_a)\n",
    "    \n",
    "    fit_time = (time.time() - start_time) / len(grid_search.cv_results_['mean_fit_time'])\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # --- Evaluate on preprocessed data ---\n",
    "    train_score = best_model.score(X_resampled_a_df, y_resampled_a)\n",
    "    test_score = best_model.score(X_test_df, y_test)\n",
    "    \n",
    "    if test_score > best_overall_score_exp_a:\n",
    "        best_overall_score_exp_a = test_score\n",
    "        best_overall_model_exp_a = name\n",
    "        best_overall_params_exp_a = grid_search.best_params_\n",
    "        best_model_for_final_eval_exp_a = best_model\n",
    "\n",
    "    y_pred_a = best_model.predict(X_test_df)\n",
    "    \n",
    "    # Calculate probabilities and store them in the dictionary for the ROC curve\n",
    "    y_pred_proba_a = best_model.predict_proba(X_test_df)[:, 1]   # Get the probabilities for the positive class\n",
    "    y_pred_probas[name] = y_pred_proba_a\n",
    "\n",
    "    # Calculating metrics using the current predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred_a)\n",
    "    recall = recall_score(y_test, y_pred_a)\n",
    "    precision = precision_score(y_test, y_pred_a, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_a, zero_division=0)\n",
    "\n",
    "    # --- Getting Optimal Threshold ---\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_a)\n",
    "    # Calculate F1-score for each threshold\n",
    "    f_scores = [f1_score(y_test, (y_pred_proba_a >= t).astype(int)) for t in thresholds]\n",
    "\n",
    "    # Finding the threshold that gives the highest F1-score\n",
    "    optimal_threshold = thresholds[np.argmax(f_scores)]\n",
    "\n",
    "    # --- Applying the optimal threshold to get new predictions ---\n",
    "    y_pred_optimal_a = (y_pred_proba_a >= optimal_threshold).astype(int)\n",
    "\n",
    "    # --- Calculating metrics with the new predictions ---\n",
    "    accuracy_optimized = accuracy_score(y_test, y_pred_optimal_a)\n",
    "    recall_optimized = recall_score(y_test, y_pred_optimal_a)\n",
    "    precision_optimized = precision_score(y_test, y_pred_optimal_a, zero_division=0)\n",
    "    f1_optimized = f1_score(y_test, y_pred_optimal_a, zero_division=0)\n",
    "\n",
    "    results.append([name, train_score, test_score, fit_time, accuracy, recall, precision, f1])\n",
    "    results_optimized.append([name, train_score, test_score, fit_time, accuracy_optimized, recall_optimized, precision_optimized, f1_optimized])\n",
    "\n",
    "# Select the probabilities of the best model for the ROC curve\n",
    "y_pred_probas_exp_a = y_pred_probas[best_overall_model_exp_a]\n",
    "\n",
    "# Creating the results DataFrame with the new columns\n",
    "results_df = pd.DataFrame(results, columns=['model', 'train score', 'test score', 'average fit time', 'accuracy', 'recall', 'precision', 'f1_score'])\n",
    "results_df.set_index('model', inplace=True)\n",
    "\n",
    "# Creating the results DataFrame optimized with the new columns\n",
    "results_df_opt = pd.DataFrame(results_optimized, columns=['model', 'train score', 'test score', 'average fit time', 'accuracy', \n",
    "                                                          'recall', 'precision', 'f1_score'])\n",
    "results_df_opt.set_index('model', inplace=True)\n",
    "\n",
    "# Saving the df for the final analysis\n",
    "results_df_exp_a = results_df\n",
    "results_df_exp_a_opt = results_df_opt\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"F1-Score Non-Optimized Models results\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Beautifying the DataFrame\n",
    "styled_df_exp_a = (results_df_exp_a.style\n",
    "    .background_gradient(cmap='Blues')\n",
    "    .format({\n",
    "        'train score': '{:.2%}',\n",
    "        'test score': '{:.2%}',\n",
    "        'accuracy': '{:.2%}',\n",
    "        'recall': '{:.2%}',\n",
    "        'precision': '{:.2%}',\n",
    "        'f1_score': '{:.2%}',\n",
    "        'average fit time': '{:.2f}s'\n",
    "    })\n",
    "    .highlight_max(subset=['test score', 'accuracy', 'recall', 'precision', 'f1_score'], axis=0, color='green') # Highlight the best scores\n",
    "    .set_caption(\"Model Performance Metrics\")\n",
    ")\n",
    "\n",
    "display(styled_df_exp_a)\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"F1-Score Optimized Models results\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "\n",
    "# Beautifying the DataFrame\n",
    "styled_df_exp_a_opt = (results_df_exp_a_opt.style\n",
    "    .background_gradient(cmap='Blues')\n",
    "    .format({\n",
    "        'train score': '{:.2%}',\n",
    "        'test score': '{:.2%}',\n",
    "        'accuracy': '{:.2%}',\n",
    "        'recall': '{:.2%}',\n",
    "        'precision': '{:.2%}',\n",
    "        'f1_score': '{:.2%}',\n",
    "        'average fit time': '{:.2f}s'\n",
    "    })\n",
    "    .highlight_max(subset=['test score', 'accuracy', 'recall', 'precision', 'f1_score'], axis=0, color='green') # Highlight the best scores\n",
    "    .set_caption(\"Model Performance Metrics\")\n",
    ")\n",
    "\n",
    "display(styled_df_exp_a_opt)\n",
    "\n",
    "# Print the overall best model and its parameters\n",
    "print(\"=\"*90)\n",
    "print(\"Overall Best Model and Parameters using F1-Score criteria (non-optimized)\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Best Model: {best_overall_model_exp_a}\")\n",
    "print(f\"Best Test Score: {best_overall_score_exp_a:.2%}\")\n",
    "print(f\"Best Parameters: {best_overall_params_exp_a}\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining confusion matrix with optimal threshold\n",
    "y_pred_proba_best_exp_a = best_model_for_final_eval_exp_a.predict_proba(X_test_df)[:, 1]\n",
    "optimal_threshold_a = optimal_threshold\n",
    "\n",
    "y_pred_best_exp_a_optimized = (y_pred_proba_best_exp_a >= optimal_threshold_a).astype(int)\n",
    "cm_a_optimized = confusion_matrix(y_test, y_pred_best_exp_a_optimized)\n",
    "\n",
    "# Creating DataFrame for the confusion matrix with proper labels\n",
    "cm_a_df = pd.DataFrame(cm_a_optimized,\n",
    "                     index=['Actual Negative', 'Actual Positive'],\n",
    "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
    "\n",
    "# Creating a heatmap\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(cm_a_df, annot=True, fmt='d', cmap='Blues', cbar=False, linewidths=0.5, linecolor='black')\n",
    "plt.title('Confusion Matrix Heatmap', fontsize=16)\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Getting the classification report as a dictionary\n",
    "report_dict = classification_report(y_test, y_pred_best_exp_a_optimized, output_dict=True)\n",
    "\n",
    "# Converting the dictionary to a DataFrame\n",
    "report_exp_a_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "# Formating the numerical values as percentages and add a background color\n",
    "styled_report_exp_a = (report_exp_a_df.style\n",
    "    .format({'precision': '{:.2%}',\n",
    "             'recall': '{:.2%}',\n",
    "             'f1-score': '{:.2%}',\n",
    "             'support': '{:.0f}'})\n",
    "    .background_gradient(cmap='Blues', subset=['precision', 'recall', 'f1-score'])\n",
    "    .set_caption(\"Classification Report\")\n",
    ")\n",
    "\n",
    "# Displaying the styled DataFrame\n",
    "display(styled_report_exp_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Experiment B - Address unbalanced dataset issue using Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"=\"*50)\n",
    "# Checking class distribution of the training data before undersampling\n",
    "print(\"Class distribution before undersampling:\")\n",
    "print(Counter(y_train))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Creating an undersampling object\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Fitting and applying the undersampler to the training data\n",
    "X_resampled_b, y_resampled_b = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"=\"*50)\n",
    "# Checking the class distribution after undersampling\n",
    "print(\"Class distribution after undersampling:\")\n",
    "print(Counter(y_resampled_b))\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining and fitting the preprocessor on training data\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "preprocessor_fitted = preprocessor.fit(X_train)\n",
    "\n",
    "# Preprocessing training data to be used by the resamplers\n",
    "X_train_preprocessed = preprocessor_fitted.transform(X_train)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Class distribution before undersampling:\")\n",
    "print(Counter(y_train))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Defining a dictionary of undersampling techniques\n",
    "undersamplers = {\n",
    "    'RandomUnderSampler': RandomUnderSampler(random_state=42),\n",
    "    #'NearMiss': NearMiss(version=1, n_neighbors=3),\n",
    "    #'EditedNearestNeighbours': EditedNearestNeighbours(n_neighbors=3)\n",
    "}\n",
    "\n",
    "# Looping through each undersampler and apply it to the preprocessed data\n",
    "for name, resampler in undersamplers.items():\n",
    "    print(f\"Applying {name}...\")\n",
    "    \n",
    "    # Fitting and applying the undersampler to the preprocessed training data\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    # Checking the class distribution after undersampling\n",
    "    print(f\"Class distribution after {name}:\")\n",
    "    print(Counter(y_resampled))\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the models and hyperparameters\n",
    "\n",
    "best_overall_model_exp_b = None\n",
    "best_overall_score_exp_b = 0.0\n",
    "best_overall_params_exp_b = {}\n",
    "best_model_for_final_eval_exp_b = None\n",
    "\n",
    "models = {\n",
    "    'knn': (KNeighborsClassifier(), {'knn__n_neighbors': [3, 5, 7]}),\n",
    "    'logisticregression': (LogisticRegression(max_iter=1000, class_weight='balanced'), {'logisticregression__C': [0.1, 1, 10, 50]}),\n",
    "    'svc': (SVC(probability=True, class_weight='balanced'), {'svc__C': [0.1, 1, 10], 'svc__kernel': ['linear', 'rbf']}),\n",
    "    'decisiontreeclassifier': (DecisionTreeClassifier(class_weight='balanced'), {'decisiontreeclassifier__max_depth': [5, 10, 12, 15]})\n",
    "}\n",
    "\n",
    "results = []\n",
    "results_optimized = []\n",
    "\n",
    "# Creating dictionaries to store the predicted probabilities for each experiment\n",
    "y_pred_probas = {}\n",
    "\n",
    "# Function to get feature names after preprocessing\n",
    "def get_feature_names(preprocessor, X):\n",
    "    output_features = []\n",
    "    \n",
    "    # Check if preprocessor has 'num' and 'cat' named transformers\n",
    "    if 'num' in preprocessor.named_transformers_:\n",
    "        output_features.extend(preprocessor.named_transformers_['num'].get_feature_names_out())\n",
    "    \n",
    "    if 'cat' in preprocessor.named_transformers_:\n",
    "        output_features.extend(preprocessor.named_transformers_['cat'].get_feature_names_out())\n",
    "        \n",
    "    return output_features\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        (name, model)\n",
    "    ])\n",
    "\n",
    "    # I will select de best model based on F1-score\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    \n",
    "    # Performing grid search\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=-1, scoring=f1_scorer)\n",
    "    \n",
    "    # Fitting the model and time it\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_resampled_b, y_resampled_b)\n",
    "    fit_time = (time.time() - start_time) / len(grid_search.cv_results_['mean_fit_time'])\n",
    "    \n",
    "    # Getting the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluating on training and test sets\n",
    "    train_score = best_model.score(X_train, y_train)\n",
    "    test_score = best_model.score(X_test, y_test)\n",
    "\n",
    "    # Checking if the current model is the best overall\n",
    "    if test_score > best_overall_score_exp_b:\n",
    "        best_overall_score_exp_b = test_score\n",
    "        best_overall_model_exp_b = name\n",
    "        best_overall_params_exp_b = grid_search.best_params_\n",
    "        best_model_for_final_eval_exp_b = best_model\n",
    "\n",
    "    # Making predictions and calculate metrics for current model\n",
    "    y_pred_b = best_model.predict(X_test)\n",
    "       \n",
    "    # Calculating probabilities and store them in the dictionary for the ROC curve\n",
    "    y_pred_proba_b = best_model.predict_proba(X_test)[:, 1]  # Get the probabilities for the positive class\n",
    "    y_pred_probas[name] = y_pred_proba_b\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_b)\n",
    "    recall = recall_score(y_test, y_pred_b)\n",
    "    precision = precision_score(y_test, y_pred_b, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_b, zero_division=0)\n",
    "\n",
    "    # Getting Optimal Threshold\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_b)\n",
    "    distance_to_corner = np.sqrt(fpr**2 + (1 - tpr)**2)\n",
    "    optimal_threshold_index = np.argmin(distance_to_corner)\n",
    "    optimal_threshold = thresholds[optimal_threshold_index]\n",
    "\n",
    "    # Applying the optimal threshold to get new predictions\n",
    "    y_pred_optimal_b = (y_pred_proba_b >= optimal_threshold).astype(int)\n",
    "\n",
    "    # Calculating metrics with the new predictions\n",
    "    accuracy_optimized = accuracy_score(y_test, y_pred_optimal_b)\n",
    "    recall_optimized = recall_score(y_test, y_pred_optimal_b)\n",
    "    precision_optimized = precision_score(y_test, y_pred_optimal_b, zero_division=0)\n",
    "    f1_optimized = f1_score(y_test, y_pred_optimal_b, zero_division=0)\n",
    "\n",
    "    # Getting Features Importance\n",
    "    print(f\"\\n--- Feature Importance for {name.upper()} ---\")\n",
    "    \n",
    "    # Getting the features names\n",
    "    feature_names = get_feature_names(best_model.named_steps['preprocessor'], X_test)\n",
    "    \n",
    "    # Conditional logic for different model types\n",
    "    if name in ['logisticregression', 'svc']:\n",
    "        # For linear models, use absolute coefficients\n",
    "        if hasattr(best_model.named_steps[name], 'coef_'):\n",
    "            # Checking if coef_ is a sparse matrix and convert it to a dense array\n",
    "            coef = best_model.named_steps[name].coef_\n",
    "            if hasattr(coef, 'toarray'):\n",
    "                importances = np.abs(coef.toarray()[0])\n",
    "            else:\n",
    "                importances = np.abs(coef[0])\n",
    "    \n",
    "            importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "            \n",
    "            # Rounding the 'Importance' column to two decimal places\n",
    "            importance_df['Importance'] = importance_df['Importance'].round(2)\n",
    "            print(importance_df.sort_values(by='Importance', ascending=False).head(10))\n",
    "    \n",
    "    elif name == 'decisiontreeclassifier':\n",
    "        # For tree-based models, use feature_importances_\n",
    "        if hasattr(best_model.named_steps[name], 'feature_importances_'):\n",
    "            importances = best_model.named_steps[name].feature_importances_\n",
    "            importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "            # Round the 'Importance' column to two decimal places\n",
    "            importance_df['Importance'] = importance_df['Importance'].round(2)\n",
    "            print(importance_df.sort_values(by='Importance', ascending=False).head(10))\n",
    "   \n",
    "    # Appending the results, including the new metrics for both versions\n",
    "    results.append([name, train_score, test_score, fit_time, accuracy, recall, precision, f1])\n",
    "    results_optimized.append([name, train_score, test_score, fit_time, accuracy_optimized, recall_optimized, precision_optimized, f1_optimized])\n",
    "\n",
    "# Selecting the probabilities of the best model for the ROC curve\n",
    "y_pred_probas_exp_b = y_pred_probas[best_overall_model_exp_b]\n",
    "\n",
    "# Creating the results DataFrame with the new columns\n",
    "results_df = pd.DataFrame(results, columns=['model', 'train score', 'test score', 'average fit time', 'accuracy', 'recall', 'precision', 'f1_score'])\n",
    "results_df.set_index('model', inplace=True)\n",
    "\n",
    "# Creating the results DataFrame optimized with the new columns\n",
    "results_df_opt = pd.DataFrame(results_optimized, columns=['model', 'train score', 'test score', 'average fit time', \n",
    "                                                          'accuracy', 'recall', 'precision', 'f1_score'])\n",
    "results_df_opt.set_index('model', inplace=True)\n",
    "\n",
    "# Saving the dfs for the final analysis\n",
    "results_df_exp_b = results_df\n",
    "results_df_exp_b_opt = results_df_opt\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"F1-Score Non-Optimized Models results\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Beautifing the DataFrame\n",
    "styled_df_exp_b = (results_df_exp_b.style\n",
    "    .background_gradient(cmap='Blues')\n",
    "    .format({\n",
    "        'train score': '{:.2%}',\n",
    "        'test score': '{:.2%}',\n",
    "        'accuracy': '{:.2%}',\n",
    "        'recall': '{:.2%}',\n",
    "        'precision': '{:.2%}',\n",
    "        'f1_score': '{:.2%}',\n",
    "        'average fit time': '{:.2f}s'\n",
    "    })\n",
    "    .highlight_max(subset=['test score', 'accuracy', 'recall', 'precision', 'f1_score'], axis=0, color='green') # Highlight the best scores\n",
    "    .set_caption(\"Model Performance Metrics\")\n",
    ")\n",
    "\n",
    "display(styled_df_exp_b)\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"F1-Score Optimized Models results\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "\n",
    "# Beautifing the DataFrame\n",
    "styled_df_exp_b_opt = (results_df_exp_b_opt.style\n",
    "    .background_gradient(cmap='Blues')\n",
    "    .format({\n",
    "        'train score': '{:.2%}',\n",
    "        'test score': '{:.2%}',\n",
    "        'accuracy': '{:.2%}',\n",
    "        'recall': '{:.2%}',\n",
    "        'precision': '{:.2%}',\n",
    "        'f1_score': '{:.2%}',\n",
    "        'average fit time': '{:.2f}s'\n",
    "    })\n",
    "    .highlight_max(subset=['test score', 'accuracy', 'recall', 'precision', 'f1_score'], axis=0, color='green') # Highlight the best scores\n",
    "    .set_caption(\"Model Performance Metrics\")\n",
    ")\n",
    "\n",
    "display(styled_df_exp_b_opt)\n",
    "\n",
    "# Print the overall best model and its parameters\n",
    "print(\"=\"*90)\n",
    "print(\"Overall Best Model and Parameters using F1-Score criteria (non-optimized)\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Best Model: {best_overall_model_exp_b}\")\n",
    "print(f\"Best Test Score: {best_overall_score_exp_b:.2%}\")\n",
    "print(f\"Best Parameters: {best_overall_params_exp_b}\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining confusion matrix with optimal threshold\n",
    "y_pred_proba_best_exp_b = best_model_for_final_eval_exp_b.predict_proba(X_test)[:, 1]\n",
    "optimal_threshold_b = optimal_threshold\n",
    "\n",
    "y_pred_best_exp_b_optimized = (y_pred_proba_best_exp_b >= optimal_threshold_b).astype(int)\n",
    "cm_b_optimized = confusion_matrix(y_test, y_pred_best_exp_b_optimized)\n",
    "\n",
    "# Creating a DataFrame for the confusion matrix with proper labels\n",
    "cm_b_df = pd.DataFrame(cm_b_optimized,\n",
    "                     index=['Actual Negative', 'Actual Positive'],\n",
    "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
    "\n",
    "# Creatings a heatmap for more redability\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(cm_b_df, annot=True, fmt='d', cmap='Blues', cbar=False, linewidths=0.5, linecolor='black')\n",
    "plt.title('Confusion Matrix Heatmap', fontsize=16)\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Getting the classification report as a dictionary\n",
    "report_dict = classification_report(y_test, y_pred_best_exp_b_optimized, output_dict=True)\n",
    "\n",
    "# Converting the dictionary to a DataFrame\n",
    "report_exp_b_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "# Formating the numerical values as percentages and add a background color\n",
    "styled_report_exp_b = (report_exp_b_df.style\n",
    "    .format({'precision': '{:.2%}',\n",
    "             'recall': '{:.2%}',\n",
    "             'f1-score': '{:.2%}',\n",
    "             'support': '{:.0f}'})\n",
    "    .background_gradient(cmap='Blues', subset=['precision', 'recall', 'f1-score'])\n",
    "    .set_caption(\"Classification Report\")\n",
    ")\n",
    "\n",
    "# Display the styled DataFrame\n",
    "display(styled_report_exp_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.5 Confusion Matrix explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Accuracy\n",
    "It measures the total percentage of predictions that the model got right. \n",
    "This includes both correctly predicted positive cases and correctly predicted negative cases.\n",
    "##### Accuracy = CorrectPredictions / TotalPredictions\n",
    "###### ================================================\n",
    "###### Precision\n",
    "Precision focuses on the model's positive predictions. \n",
    "It answers the question: \"Of all the times the model predicted a positive outcome, how many were actually correct?\"\n",
    "##### Precision = TruePositives / TruePositives + FalsePositives\n",
    "A high precision score means the model has a low number of false positives.\n",
    "###### ================================================\n",
    "##### Recall\n",
    "Recall/sensitivity, focuses on the actual positive cases. \n",
    "It answers the question: \"Of all the actual positive outcomes, how many did the model correctly identify?\"\n",
    "##### Recall = TruePositives / TruePositives + FalseNegatives\n",
    "A high recall score means the model has a low number of false negatives. \n",
    "###### ================================================\n",
    "###### F1-Score\n",
    "The F1-Score is a single metric that provides a balance between precision and recall. \n",
    "It is the harmonic mean of the two, making it a more reliable metric than accuracy for imbalanced datasets.\n",
    "##### F1Score = 2  PrecisionRecall / Precision+Recall\n",
    "A high F1-score indicates that the model is performing well on both precision and recall, \n",
    "meaning it is both accurate in its positive predictions and effective at finding most of the actual positive cases.\n",
    "###### ================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiments results and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculations for Experiment A ---\n",
    "fpr_a, tpr_a, thresholds_a = roc_curve(y_test, y_pred_probas_exp_a) # Using the probabilities of the best model for Experiment A\n",
    "roc_auc_a = roc_auc_score(y_test, y_pred_probas_exp_a)\n",
    "\n",
    "distance_a = np.sqrt(fpr_a**2 + (1 - tpr_a)**2)\n",
    "optimal_threshold_index_a = np.argmin(distance_a)\n",
    "optimal_threshold_a = thresholds_a[optimal_threshold_index_a]\n",
    "\n",
    "# --- Calculations for Experiment B ---\n",
    "fpr_b, tpr_b, thresholds_b = roc_curve(y_test, y_pred_probas_exp_b) # Using the probabilities of the best model for Experiment B\n",
    "roc_auc_b = roc_auc_score(y_test, y_pred_probas_exp_b)\n",
    "\n",
    "distance_b = np.sqrt(fpr_b**2 + (1 - tpr_b)**2)\n",
    "optimal_threshold_index_b = np.argmin(distance_b)\n",
    "optimal_threshold_b = thresholds_b[optimal_threshold_index_b]\n",
    "\n",
    "# --- Plotting Both ROC Curves and Optimal Thresholds ---\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot ROC curve for Experiment A\n",
    "plt.plot(fpr_a, tpr_a, color='darkorange', lw=2, label=f'Exp A ROC (AUC = {roc_auc_a:.2f})')\n",
    "# Add a marker for the optimal threshold of Experiment A\n",
    "plt.scatter(\n",
    "    fpr_a[optimal_threshold_index_a], \n",
    "    tpr_a[optimal_threshold_index_a], \n",
    "    color='red', \n",
    "    marker='o', \n",
    "    s=100, \n",
    "    label=f'Exp A Opt. Threshold: {optimal_threshold_a:.2f}'\n",
    ")\n",
    "\n",
    "# Plot ROC curve for Experiment B\n",
    "plt.plot(fpr_b, tpr_b, color='forestgreen', lw=2, label=f'Exp B ROC (AUC = {roc_auc_b:.2f})')\n",
    "# Add a marker for the optimal threshold of Experiment B\n",
    "plt.scatter(\n",
    "    fpr_b[optimal_threshold_index_b], \n",
    "    tpr_b[optimal_threshold_index_b], \n",
    "    color='purple', \n",
    "    marker='o', \n",
    "    s=100, \n",
    "    label=f'Exp B Opt. Threshold: {optimal_threshold_b:.2f}'\n",
    ")\n",
    "\n",
    "# Plot the random guess line\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random guess')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show();\n",
    "\n",
    "# Save the plots to a file\n",
    "plt.savefig(\"data/ROCs_plot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_end_time = time.time()\n",
    "notebook_elapsed_time = notebook_end_time - notebook_start_time\n",
    "print(f\"Total Elapsed time : {notebook_elapsed_time/60:.2f} Minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Findings and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments results and key findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall technical findings**\n",
    "- For these experiments using accuracy as key evaluation metric is very misleading and not recommended becuae of the very unbalanced data set 90% for the majority class. Considering the business context, identification of bank's clients that would accept a marketing campaing promotion (in this case a good interest rate deposit) the most important evaluation metrics are Precision, Recall and F1-Score overall but even more important in the context of the minority class (which in this case is the accepted promotion). Considering the above the experiments were designed to optimise(increase) F1-Score, which is a better metric for unbalanced dataset because is representing the optimal point between precision and recall metrics.\n",
    "- Models selection and optimization techniques considered two stages:\n",
    "   1. Selecting the best model using GridSearch and adjusting the hyperparameters for the best F1-score\n",
    "   2. Finding the optimal probability threeshold for each model comparing the results for **F1-Score maximization**\n",
    "- Techniques used to deal with unbalanced nature of the dataset and its size in Experiment A:\n",
    "   - **Oversampling** using SMOTE\n",
    "   - **UnderSampling** using: RandomUnderSampler, NearMiss-version1, EditedNearestNeighbours\n",
    "   - **class_weight='balanced'** for all models where applicable\n",
    "   - **Adjustable dataset sampling for SVM** which is the most cumputational expenside model (started with smaller test samples to run more kernels and after selecting the best kernel by increased the test sample\n",
    "- During the experimentation I've concluded that oversampling the minority class created a better dataset than undersampling the majority class. The results are showing that all models performs better on the oversampled dataset. This is clearly observed by comparing the F1-Score, Precision and Recall metrics for both experiments. I have used Confusion Matrix, and Classification Report for a deeper dive into the performance of each model\n",
    "  - The **best models from Experiment-A and also overall** is: **LogisticRegression Model** with below with below results:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"data/Experiment-A_res.png\" alt=\"Experiment-A Results\">\n",
    "</div>\n",
    "    \n",
    "  - The best models for Experiment-B is: **Support Vector Machine (SVM)** with below results:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"data/Experiment-B_res.png\" alt=\"Experiment-B Results\">\n",
    "</div>\n",
    "     \n",
    "  - For each models (where reasonable possible) I've also extracted the most relevant features:\n",
    "- In both experiments I've managed to get a pretty good ROC score 0.93 for Experiment-B and 0.91 for Experiment-A. Nevertheless, ROC for Experiment-B is slighly better but only because the optimization was performed for F1-Score and not ROC. However ROC, confirms that bots experiments show good good results.\n",
    "  \n",
    "<div align=\"center\">\n",
    "  <img src=\"data/ROCs_Plot.jpg\" alt=\"ROCs Results\">\n",
    "</div>\n",
    "\n",
    "- Nest step is to try new more powerfull models to further improve F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business related findings and recommendations:**\n",
    "\n",
    "With the **selected Logistic Regression model** if the bank intend to launcg a very targets campaing that optimizes the campaing cost then they should be able to **identify a list of clients that should be targeted by the new promotion campaign**. \n",
    "\n",
    "The model predict that **45.5% of the clients in this list will accept the promotion!**\n",
    "This will represent **60.59%** of the total number of all clients in the database that would accept the promotion.\n",
    "This is explained by below Classification Reports that shown the **Precision** and **Recall** as **%** for the **Class 1 (Clients that would accept the promotion)**:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"data/Experiment-A_Confusion&Classification.jpg\" alt=\"Experiment-A Results\">\n",
    "</div>\n",
    "\n",
    "If the bank is not very concerned with the overall campaign cost and want to maximize the total number of clients that will accept the promotion, then they can use the model to select from bank database all clients that should not be part of the campaing. This will allow the bank to exclude from the campaing the clients that would not take the promotion anyway. This approach will give the bank all clinets that would not accpt the promotion with a probability of **95.95%**. \n",
    "In this scenario the bank will have a bigger list of clients than in the first scenario. In this list the percentace of clinets that will accept the promotion is significantly bigger than **45.5%**. In this case the bank would miss only **5%** of the total clients that are in the database but not selected for the marketing campaing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step to improve the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nest step is to try new more powerfull models to further improve F1-Score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
